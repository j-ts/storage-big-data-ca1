{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b131f231",
   "metadata": {},
   "source": [
    "# Storage Solutions for Big Data - CA1\n",
    "\n",
    "\n",
    "The assessment CA 1 by **Yulianna Tsaruk**.\\\\\\n\",\n",
    "Programme Title: Higher Diploma in Science in AI Applications\n",
    "Module Title: Storage Solutions for Big Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Code contents:\n",
    "- **Exploratory Data Analysis & Processing (this file)**\n",
    "- **[Training model and Usage Example](./2_training.ipynb)**\n",
    "\n",
    "\n",
    "\n",
    "## Intoduction\n",
    "\n",
    "Fir this project I'm using HDFS (Hadoop Distributed File System) as a primary storage system used by Apache Spark for processing, and an interface for Apache Spark in Python called PySpark.\n",
    "\n",
    "In this file, I will load the dataset, process it and save as in Apache Parquet â€“ a column-oriented data storage format in the Apache Hadoop ecosystem, designed for efficient data storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90256178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304adcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark instances\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import to_date, dayofmonth, month, year, col, explode, \\\n",
    "                unix_timestamp, when, regexp_replace, mean, concat_ws, \\\n",
    "                dayofweek, udf\n",
    "from pyspark.sql.types import FloatType, BooleanType, StringType\n",
    "\n",
    "# import additional libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bedf9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 18:50:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark session with configurations\n",
    "spark = (SparkSession.builder \\\n",
    "    .appName(\"Tokyo Airbnb Analysis\")\n",
    "    # hardware-related configs, comment it if not needed for your machine.\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.network.timeout\", \"600s\") \n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\n",
    "    \n",
    "    # to output more\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76927a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating same output equivalent to the pandas.DataFrame.info() method  \n",
    "def print_dataframe_info(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Print basic information about data like column names, null counts, and data types for a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The Spark DataFrame to be analyze.\n",
    "    \"\"\"\n",
    "    # DataFrame shape\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "\n",
    "    # Collect column names and their data types\n",
    "    schema_info = [(field.name, field.dataType) for field in df.schema.fields]\n",
    "    out_ = []\n",
    "    for column, dtype in schema_info:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        out_.append({'Column': column, 'Nulls': null_count, 'Type': dtype.simpleString()})\n",
    "    \n",
    "    print(pd.DataFrame(out_))\n",
    "    print()\n",
    "    print(f\"\\tA dataset shape: {total_rows} rows, {total_cols} columns.\")\n",
    "\n",
    "pd.set_option('display.max_rows', None) # show all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23996547",
   "metadata": {},
   "source": [
    "## Load 1st dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a309a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+--------------+--------------+--------------+\n",
      "|listing_id|      date|available|     price|adjusted_price|minimum_nights|maximum_nights|\n",
      "+----------+----------+---------+----------+--------------+--------------+--------------+\n",
      "|    197677|2023-06-29|        f|$11,000.00|    $11,000.00|             3|          1125|\n",
      "|    197677|2023-06-30|        f|$11,000.00|    $11,000.00|             3|          1125|\n",
      "|    197677|2023-07-01|        f|$11,000.00|    $11,000.00|             3|          1125|\n",
      "|    197677|2023-07-02|        f|$11,000.00|    $11,000.00|             3|          1125|\n",
      "|    197677|2023-07-03|        f|$11,000.00|    $11,000.00|             3|          1125|\n",
      "+----------+----------+---------+----------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# location of 1st file in Hadoop\n",
    "dataset_path = \"/user1/dataset/calendar.csv\" \n",
    "\n",
    "# load data\n",
    "df_calendar = spark.read.csv(dataset_path, header=True, # 1st line is a header\n",
    "                             inferSchema=True           # detect data types automatically\n",
    "                            )\n",
    "df_calendar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb1848",
   "metadata": {},
   "source": [
    "### Explore and Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d246c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Column  Nulls    Type\n",
      "0      listing_id      0  bigint\n",
      "1            date      0    date\n",
      "2       available      0  string\n",
      "3           price      0  string\n",
      "4  adjusted_price      0  string\n",
      "5  minimum_nights    332     int\n",
      "6  maximum_nights    332     int\n",
      "\n",
      "\tA dataset shape: 4078413 rows, 7 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Nulls and types summary\n",
    "print_dataframe_info(df_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722e6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+-------------+--------------+------------------+------------------+\n",
      "|summary|          listing_id|available|        price|adjusted_price|    minimum_nights|    maximum_nights|\n",
      "+-------+--------------------+---------+-------------+--------------+------------------+------------------+\n",
      "|  count|             4078413|  4078413|      4078413|       4078413|           4078081|           4078081|\n",
      "|   mean|2.520073870723633...|     null|         null|          null|2.7664423536462373| 676.3645109059873|\n",
      "| stddev|3.745944660493916...|     null|         null|          null|12.338796688561727|449.44182861714916|\n",
      "|    min|              197677|        f|$1,000,000.00| $1,000,000.00|                 1|                 1|\n",
      "|    max|  923132709196905769|        t|  $999,999.00|   $999,999.00|              1000|              1125|\n",
      "+-------+--------------------+---------+-------------+--------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "df_calendar.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396aa746",
   "metadata": {},
   "source": [
    "Some variables are wrong dtype. For example, we can't see mean of price column, because values are string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324036b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11175 properties in this dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>count</th>\n",
       "      <th>days_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4050925</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12597472</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19413667</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>742241466430561437</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>806171362910072416</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           listing_id  count  days_count\n",
       "0             4050925     33          33\n",
       "1            12597472    339         339\n",
       "2            19413667    339         339\n",
       "3  742241466430561437    339         339\n",
       "4  806171362910072416    339         339"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check amount of unique values in the 'listing_id' column\n",
    "listing_gr = df_calendar.groupBy(\"listing_id\").count()\n",
    "\n",
    "print('There are', listing_gr.count(), 'properties in this dataset.')\n",
    "\n",
    "pandas_df_listing = listing_gr.orderBy(col(\"count\")).toPandas()\n",
    "pandas_df_listing['days_count'] = pandas_df_listing['count']\n",
    "result = pandas_df_listing.groupby('days_count').size().reset_index(name='properties_count')\n",
    "pandas_df_listing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f3ca8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>count</th>\n",
       "      <th>days_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4050925</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12597472</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19413667</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>742241466430561437</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>806171362910072416</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>772140258073141388</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           listing_id  count  days_count\n",
       "0             4050925     33          33\n",
       "1            12597472    339         339\n",
       "2            19413667    339         339\n",
       "3  742241466430561437    339         339\n",
       "4  806171362910072416    339         339\n",
       "5  772140258073141388    339         339"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df_listing.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bde465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_count</th>\n",
       "      <th>properties_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>339</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365</td>\n",
       "      <td>11169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days_count  properties_count\n",
       "0          33                 1\n",
       "1         339                 5\n",
       "2         365             11169"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d813ea0",
   "metadata": {},
   "source": [
    "1 property has data only for 33 days, while most of other properties obtain data for a whole year (365 days). I will drop this property, and for other 5 that has data only for 339 days, I will impute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab04024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12597472,\n",
       " 19413667,\n",
       " 742241466430561437,\n",
       " 806171362910072416,\n",
       " 772140258073141388]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_to_correct = list(pandas_df_listing[pandas_df_listing['count'] == 339]['listing_id'])\n",
    "listing_to_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320b62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows for 'listing_id' == 4050925 because too much data is missing\n",
    "df_calendar = df_calendar.filter(col(\"listing_id\") != 4050925)\n",
    "# For other listings, that has data only for 339 days out of 365, I will impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2bb0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, first, last, monotonically_increasing_id, lag, lead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbbd5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[listing_id: bigint, date: date, available: string, price: string, adjusted_price: string, minimum_nights: int, maximum_nights: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = df_calendar.filter(col(\"listing_id\").isin(listing_to_correct))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e0f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a monotonically increasing ID for row ordering\n",
    "imputed_data_interpolate = filtered_data.withColumn(\"row_id\", monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e42a04ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----------+--------------+--------------+--------------+------+\n",
      "|listing_id|      date|available|     price|adjusted_price|minimum_nights|maximum_nights|row_id|\n",
      "+----------+----------+---------+----------+--------------+--------------+--------------+------+\n",
      "|  12597472|2023-06-29|        f|$60,000.00|    $54,000.00|             2|          1125|     0|\n",
      "|  12597472|2023-06-30|        f|$60,000.00|    $54,000.00|             2|          1125|     1|\n",
      "|  12597472|2023-07-01|        f|$70,000.00|    $63,000.00|             2|          1125|     2|\n",
      "+----------+----------+---------+----------+--------------+--------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputed_data_interpolate.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144d109e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_correct_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49568/1415699288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_correct_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to_correct_df' is not defined"
     ]
    }
   ],
   "source": [
    "to_correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_calendar = df_calendar.withColumn(\"date\", to_date(df_calendar.date, 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options in col 'available'\n",
    "df_calendar.select('available').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38cccc",
   "metadata": {},
   "source": [
    "It's wotrth to note that, though price has a US dollar sign, it is in Japanese Yen and a sign must be removed in order to convert data to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e3712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check min/max nights values\n",
    "#df_calendar.filter(col('minimum_nights')>=90).groupBy(\"minimum_nights\").count().sort(col('count').desc())\n",
    "nights_df = df_calendar.select(col('minimum_nights'), col('maximum_nights')).toPandas()\n",
    "nights_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3900c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46444a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nights_df.plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df_calendar_new = df_calendar \\\n",
    "    .withColumn(\"available\", when(col(\"available\") == \"t\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"price\", regexp_replace(col(\"price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "    .withColumn(\"adjusted_price\", regexp_replace(col(\"adjusted_price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "    .withColumn(\"date_unix\", unix_timestamp(col(\"date\")))\n",
    "    #.withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "    #.withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    #.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "\n",
    "    # .orderBy([\"date\", \"listing_id\"])\n",
    "\n",
    "    #.withColumn(\"days_since\", datediff(col(\"date\"), lit(\"2023-07-29\"))) \\ # new col with day since last update\n",
    "    #.withColumn(\"minimum_nights\", col(\"minimum_nights\").cast(IntegerType())) \\\n",
    "    #.withColumn(\"maximum_nights\", col(\"maximum_nights\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if there's a difference in cols \"price\" and \"adjusted_price\"\n",
    "df_with_diff = df_calendar_new.withColumn(\"price_difference\", col(\"price\") - col(\"adjusted_price\"))\n",
    "\n",
    "# Filter rows where price_difference is not zero\n",
    "rows_with_difference = df_with_diff.filter(col(\"price_difference\") != 0)\n",
    "\n",
    "# count how many rows with differences\n",
    "rows_with_difference.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a257c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_with_difference.filter(col('adjusted_price')>col('price')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708248db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_with_difference.filter(col('adjusted_price')<col('price')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee368b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_new.filter(col('adjusted_price')==col('price')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff0054",
   "metadata": {},
   "source": [
    "I save 'adjusted_price' col name to drop it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alongside with 'min/maximum_nights' which doesn't look correct\n",
    "col_to_drop = ['price', 'minimum_nights', 'maximum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_new.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ccf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_calendar_new.select(col('price')).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_price = df_calendar_new.select(col('price')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_price.groupby('price').size().reset_index(name='price_count') \\\n",
    ".sort_values( 'price', ascending=False).head()#.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7dce9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corrected.filter(\n",
    "    (col('listing_id') == 43547243) &\n",
    "    (col('listing_id') == 8108276) &\n",
    "    (col('listing_id') == 43547291)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a4bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9390d866",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcccfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "quartiles = df_calendar_new.approxQuantile(\"price\", [0.25, 0.75], 0.05)\n",
    "Q1, Q3 = quartiles[0], quartiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for the outliers\n",
    "lower_bound = df_calendar_new.select(min(col('price')).alias('lowest_price')).first()['lowest_price']\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_new.filter((col('price') > upper_bound)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.filter(col('listing_id') == 1732795).filter(col('date')=='2023-12-29').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_corrected = df_calendar_new.withColumn(\"price\", \n",
    "                             when(col(\"price\") < lower_bound, lower_bound)\n",
    "                             .when(col(\"price\") > upper_bound, None)\n",
    "                             .otherwise(col(\"price\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected.where(col(\"price\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060867ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142fc785",
   "metadata": {},
   "source": [
    "## Load 2nd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = spark.read.csv(\"/user1/dataset/listings.csv\",\n",
    "    header=True, # 1st line is a header\n",
    "    quote='\"',  \n",
    "    escape='\"', \n",
    "    multiLine=True,  # Handles new lines in fields\n",
    "    inferSchema=True,  # detect data types automatically\n",
    "    ignoreLeadingWhiteSpace=True,  # Ignoring white space in a line\n",
    "    ignoreTrailingWhiteSpace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb40a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output above is messy, let's print it pandas' df\n",
    "df_list.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7151f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if everithing loaded correctly through schema\n",
    "df_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cc93d",
   "metadata": {},
   "source": [
    "From this dataset I'll take some info to complete my 1st one. Potentially useful columns are:\n",
    "* neighbourhood_cleansed\n",
    "* host_identity_verified\n",
    "* location (latitude/longitude)\n",
    "* property_type\n",
    "* instant_bookable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebe05e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.select('property_type').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbc907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.select('room_type').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308422a",
   "metadata": {},
   "source": [
    "After checking unique values, I see that the feature I want is called 'room_type', while 'property_type' consist of marketing names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfae212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_dataframe_info(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaaefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check unique values in the 'id' column\n",
    "unique_ids_list = df_list.select(\"id\").distinct()\n",
    "\n",
    "'Unique IDs:', unique_ids_list.count(), unique_ids_list.count()-unique_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bdc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "acad24b9",
   "metadata": {},
   "source": [
    "df_calendar_ids = df_calendar.select(col(\"listing_id\").alias(\"id\")).distinct()\n",
    "\n",
    "# Select \"id\" from df_list\n",
    "df_list_ids = df_list.select(\"id\").distinct()\n",
    "\n",
    "# Find rows in df_calendar_ids that don't have a match in df_list_ids\n",
    "df_list_ids.join(df_calendar_ids, \"id\", \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ac24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [\n",
    "    'id',\n",
    "    'neighbourhood_cleansed',\n",
    "    'room_type',\n",
    "    'host_identity_verified',\n",
    "    'instant_bookable',\n",
    "]\n",
    "new_df = df_list.select(selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new df with selected_cols and df_calendar on col id and listing_id\n",
    "merged_df = new_df.join(df_calendar_new, new_df.id == df_calendar_new.listing_id, \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a778d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing dtypes\n",
    "\n",
    "df = merged_df \\\n",
    "    .withColumn(\"host_identity_verified\", when(col(\"host_identity_verified\") == \"t\", True).otherwise(False).cast(BooleanType())) \\\n",
    "    .withColumn(\"instant_bookable\", when(col(\"instant_bookable\") == \"t\", True).otherwise(False).cast(BooleanType()))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676570fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e87f672c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f8314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"available\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef197270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b107c99",
   "metadata": {},
   "source": [
    "df_busy_times = df.where(col(\"available\") == 0) \\\n",
    "                  .groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")) \\\n",
    "                  .count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busy_times = df.where(col(\"available\") == False) \\\n",
    "                  .groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")) \\\n",
    "                  .count() \\\n",
    "                  .orderBy(\"year\", \"month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15213de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70e9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df_busy_times.toPandas()\n",
    "pandas_df.sort_values(['year', 'month', 'year'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06038c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df.plot(x='month', y='count', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'month+year' column, calculate the mean of 'price',\n",
    "# and order the results by 'year-month\n",
    "\n",
    "df_price = df.where(col(\"price\") > 0) \\\n",
    "                  .withColumn(\"year_month\", concat_ws(\"-\", year(\"date\"), month(\"date\"))) \\\n",
    "                  .groupBy(\"year_month\") \\\n",
    "                  .agg(mean(\"price\").alias(\"mean\")) \\\n",
    "                  .orderBy('year_month').toPandas()\n",
    "\n",
    "# df_price.sort_values(['year', 'month', 'year'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('id', 'price').distinct().sort(col(\"price\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('id')== 561785108651423732).select('date', 'price', 'available').sort(col(\"price\").desc(), 'date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['year_month'] = df_price['year_month'].astype('period[M]')\n",
    "df_price.sort_values('year_month', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16974f58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_price.plot(x='year_month', y='mean', kind='bar'), df_price.plot(kind='line', x='year_month', y='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7301af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9b9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_calendar.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a364eb",
   "metadata": {},
   "source": [
    "## Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78bd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706eedc",
   "metadata": {},
   "source": [
    "I'm adding new features regarding date, so ML algorithm will be able to find dependencies easier:\n",
    "- if date is a weekend\n",
    "- if date is a holiday\n",
    "\n",
    "I want to use a pipeline, I need this process to be integrated into the pipeline as transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86315e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Custom Transformer with Weekend & Holiday Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34082562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if date is a weekend\n",
    "df_date = df.withColumn(\"weekends\", dayofweek(col(\"date\")).isin([6, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6738e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for holiday detection I use holidays module and user-defined function\n",
    "jp_holidays = holidays.Japan()\n",
    "\n",
    "def is_holiday(date):\n",
    "    return date in jp_holidays\n",
    "\n",
    "holiday_udf = udf(is_holiday, BooleanType())\n",
    "\n",
    "df_date = df_date.withColumn(\"holiday\", holiday_udf(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc2755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f37188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date = df_date.sort('date')\n",
    "# to use it as categorical feature\n",
    "df_date = df_date.withColumn(\"date\", col('date').cast(StringType()))\n",
    "df_date.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_to_drop = ['price', 'id', 'maximum_nights','listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe_info(df_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd9034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cleaning up \n",
    "# deleting cols that won;t be used for training\n",
    "col_to_drop += ['id', 'maximum_nights', 'date']\n",
    "#col_to_drop = list(set(col_to_drop))\n",
    "# print(df.columns, col_to_drop)\n",
    "df_model = df_date.drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.withColumnRenamed('adjusted_price', 'price')\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79c5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save DataFrame to HDFS in Parquet format\n",
    "df_model.write.parquet(\"/user1/dataset/db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bace5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.unpersist()\n",
    "df_model= spark.read.parquet(\"/user1/dataset/db/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bed18c",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae74f002",
   "metadata": {},
   "source": [
    "dataset_path = \"/user1/dataset/calendar.csv\"\n",
    "# Load and preprocess the data\n",
    "#df = spark.read.csv(dataset_path, header=True, inferSchema=True)\n",
    "\n",
    "#df = df.withColumn(\"price\", regexp_replace(col(\"price\"), \"[\\$,]\", \"\").cast(FloatType()))\n",
    "#df = df.withColumn(\"available\", when(col(\"available\") == \"t\", 1).otherwise(0)) \\\n",
    "#    .withColumn(\"date_unix\", unix_timestamp(\"date\"))\n",
    "\n",
    "#df = df.withColumn(\"date\", to_date(df.date, 'yyyy-MM-dd')).orderBy(col(\"date\"))\n",
    "\n",
    "\"\"\"    .withColumn(\"year\", year(\"date\")) \\\n",
    "    .withColumn(\"month\", month(\"date\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"date\"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  when, col, unix_timestamp, regexp_replace # , dayofmonth, month, year\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485e4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7b037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# To encode categorical features\n",
    "indexer = StringIndexer(inputCols=[\"room_type\", \"neighbourhood_cleansed\"], #, 'date'],\n",
    "                        outputCols=[\"type_indexed\", 'neighbourhood_indexed']#, 'date_indexed']\n",
    "                       )\n",
    "\n",
    "# Assemble continuous features individually\n",
    "price_vector = VectorAssembler(inputCols=[\"price\"], outputCol=\"price_vec\")\n",
    "date_vector = VectorAssembler(inputCols=[\"date_unix\"], outputCol=\"date_vec\")\n",
    "\n",
    "# Scale continuous features separately\n",
    "price_scaler = StandardScaler(inputCol=\"price_vec\", outputCol=\"scaled_price\", withStd=True, withMean=True)\n",
    "\n",
    "date_scaler = StandardScaler(inputCol=\"date_vec\", outputCol=\"scaled_date\", withStd=True, withMean=True)\n",
    "\n",
    "\n",
    "# Combine numerical and categorical features into a single vector\n",
    "assembler_f = VectorAssembler(\n",
    "    inputCols=['scaled_price', 'scaled_date',\n",
    "               # BOOLEAN\n",
    "               'instant_bookable', 'host_identity_verified', 'weekends', 'holiday',\n",
    "              # CATEGORICAL\n",
    "               \"type_indexed\", 'neighbourhood_indexed'],\n",
    "    outputCol=\"all_features\"\n",
    ")\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_model.randomSplit([0.7, 0.3], seed=42)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87c0fdd",
   "metadata": {},
   "source": [
    "# Create a transformation pipeline\n",
    "pipeline = Pipeline(stages=[indexer, continuous_assembler, scaler])\n",
    "\n",
    "transformer = pipeline.fit(df_model)\n",
    "transformed_df = transformer.transform(df_model)\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "#transformed_df.select(['features', 'scaled_features', 'available']).show(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b74fee08",
   "metadata": {},
   "source": [
    "# Create a transformation pipeline\n",
    "pipeline = Pipeline(stages=[indexer, continuous_assembler, scaler, assembler_f])\n",
    "\n",
    "transformer = pipeline.fit(df_model)\n",
    "transformed_df = transformer.transform(df_model)\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "#transformed_df.select(['features', 'scaled_features', 'available']).show(1)\n",
    "transformed_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b3115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d91f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(RandomForestClassifier.maxDepth, [3, 5, 8])\n",
    "                 .addGrid(RandomForestClassifier.numTrees, [100, 200, 300])\n",
    "                 .build())\n",
    "\n",
    "# Create a classifier for pipeline\n",
    "classifier = RandomForestClassifier(labelCol=\"available\",\n",
    "                                    featuresCol=\"all_features\",\n",
    "                                    maxBins=375,\n",
    "                                    seed=42\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a548257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[indexer, price_vector, date_vector, price_scaler, date_scaler, \n",
    "                            assembler_f, classifier])\n",
    "\n",
    "#pipeline.fit(train_data).transform(train_data).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0cc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")  # Replace with your preferred metric\n",
    "\n",
    "# Define and run cross-validation\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# model = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f313ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd89096b",
   "metadata": {},
   "source": [
    "# Save the pipeline and model to reuse it later\n",
    "\n",
    "pipeline_path = \"/user1/dataset/pipeline\"\n",
    "model_path = \"/user1/dataset/model\"\n",
    "\n",
    "pipeline.write().overwrite().save(pipeline_path)\n",
    "model.write().overwrite().save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a525b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"available\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb04ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classifier.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64503f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = assembler_f.getInputCols()\n",
    "#.featureImportances\n",
    "\n",
    "\n",
    "feature_importance_dict = dict(zip(feature_names, model.stages[-1].featureImportances.toArray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True),\n",
    "                  columns=['feature', 'importance'])\n",
    "imp.sort_values('importance', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.plot.barh(x='feature', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.filter(col('holiday')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5123616",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds = spark.sparkContext.getRDDStorageInfo()\n",
    "for rdd_info in rdds:\n",
    "    print(f\"RDD Id: {rdd_info.rddId}, Name: {rdd_info.name}, StorageLevel: {rdd_info.storageLevel}, Cached Partitions: {rdd_info.numCachedPartitions}, Memory Size: {rdd_info.memSize}, Disk Size: {rdd_info.diskSize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54088646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0938af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451fe0e",
   "metadata": {},
   "source": [
    "## Real World Usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb8e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#'date': [date_picker.value],\n",
    "#'property_type': [property_type_dropdown.value],\n",
    "#'neighbourhood_cleansed': [neighborhood_dropdown.value],\n",
    "#'instant_bookable': [instant_bookable_toggle.value == 'Yes'],\n",
    "#'host_identity_verified': [verified_host_toggle.value == 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bac1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964e542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e0ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad5001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c5c2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d56ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fffff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767c713",
   "metadata": {},
   "source": [
    "# REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf903a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, to_date, datediff, lit,regexp_replace\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Let's assume the data is in a CSV file named 'dataset.csv'\n",
    "df = spark.read.csv(\"/user1/dataset/calendar.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Data Preprocessing\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"days_since\", datediff(col(\"date\"), lit(\"2023-01-01\"))) \\\n",
    "       .withColumn(\"available\", when(col(\"available\") == \"t\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"price\", regexp_replace(col(\"price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "       .withColumn(\"adjusted_price\", regexp_replace(col(\"adjusted_price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "       .withColumn(\"minimum_nights\", col(\"minimum_nights\").cast(IntegerType())) \\\n",
    "       .withColumn(\"maximum_nights\", col(\"maximum_nights\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76216119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858420d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "assembler = VectorAssembler(inputCols=[\"days_since\", \"available\", \"minimum_nights\", \"maximum_nights\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# Modeling\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257669e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157e8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Prediction\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2fc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b099c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d21233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35b2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
