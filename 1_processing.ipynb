{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b131f231",
   "metadata": {},
   "source": [
    "# Storage Solutions for Big Data - CA1\n",
    "\n",
    "\n",
    "The assessment CA 1 by **Yulianna Tsaruk** \\\n",
    "Programme Title: Higher Diploma in Science in AI Applications \\\n",
    "Module Title: Storage Solutions for Big Data\n",
    "\n",
    "\n",
    "\n",
    "## Code contents:\n",
    "1. **Exploratory Data Analysis & Processing (this file)**\n",
    "2. **[Training model and Usage Example](./2_training.ipynb)**\n",
    "\n",
    "\n",
    "\n",
    "## Intoduction\n",
    "\n",
    "For this project I'm using HDFS (Hadoop Distributed File System) as the primary storage system, Apache Spark for processing with PySpark - an interface for Apache Spark in Python.\n",
    "\n",
    "In this file, I will load several files from a selected dataset, process them and store them in Apache Parquet - a highly efficient column-oriented data storage format in the Apache Hadoop ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c06a04-b0b9-4b64-9a71-f4b11230790b",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304adcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark instances\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import to_date, dayofmonth, month, year, col, explode, \\\n",
    "                unix_timestamp, when, regexp_replace, mean, concat_ws, \\\n",
    "                dayofweek, udf, min, max, desc, count\n",
    "from pyspark.sql.types import FloatType, BooleanType, StringType\n",
    "\n",
    "# import additional libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import holidays\n",
    "except ImportError:\n",
    "    # install library\n",
    "    !pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bedf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark session with configurations\n",
    "spark = (SparkSession.builder \\\n",
    "    .appName(\"Tokyo Airbnb Processing and Analysis\")\n",
    "    # hardware-related configs, comment it if not needed for your machine.\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.network.timeout\", \"600s\") \n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\n",
    "    \n",
    "    # to output more\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76927a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating same output equivalent to the pandas.DataFrame.info() method  \n",
    "def print_dataframe_info(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Print basic information about data like column names, null counts, and data types for a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The Spark DataFrame to be analyze.\n",
    "    \"\"\"\n",
    "    # DataFrame shape\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "\n",
    "    # Collect column names and their data types\n",
    "    schema_info = [(field.name, field.dataType) for field in df.schema.fields]\n",
    "    out_ = []\n",
    "    for column, dtype in schema_info:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        out_.append({'Column': column, 'Nulls': null_count, 'Type': dtype.simpleString()})\n",
    "    \n",
    "    print(pd.DataFrame(out_))\n",
    "    print()\n",
    "    print(f\"\\tA dataset shape: {total_rows} rows, {total_cols} columns.\")\n",
    "\n",
    "pd.set_option('display.max_rows', None) # show all rows for pandas df\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) # avoid scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23996547",
   "metadata": {},
   "source": [
    "## Load 1st dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e32d1f-4898-4e37-bb46-ab784c72805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to folder with dataset on HDFS\n",
    "dataset_path_hdfs = '/user1/dataset/' # must end with /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a309a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of 1st file in Hadoop\n",
    "dataset_path = dataset_path_hdfs + \"calendar.csv\" \n",
    "\n",
    "# load data\n",
    "df_calendar = spark.read.csv(dataset_path, header=True, # 1st line is a header\n",
    "                             inferSchema=True           # detect data types automatically\n",
    "                            )\n",
    "df_calendar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb1848",
   "metadata": {},
   "source": [
    "### Explore and Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulls and types summary\n",
    "print_dataframe_info(df_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df_calendar.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396aa746",
   "metadata": {},
   "source": [
    "Some variables are wrong dtype. For example, we can't see mean of price column, because values are string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324036b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check amount of unique values in the 'listing_id' column\n",
    "listing_gr = df_calendar.groupBy(\"listing_id\").count()\n",
    "unique_ids = listing_gr.count()\n",
    "print('There are', listing_gr.count(), 'properties in this dataset.')\n",
    "\n",
    "pandas_df_listing = listing_gr.orderBy(col(\"count\")).toPandas()\n",
    "pandas_df_listing['days_count'] = pandas_df_listing['count']\n",
    "result = pandas_df_listing.groupby('days_count').size().reset_index(name='properties_count')\n",
    "pandas_df_listing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab04024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listing_to_drop = list(pandas_df_listing[pandas_df_listing['count'] <= 339]['listing_id'])\n",
    "listing_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7820a-4a58-4c8e-9bf7-3980a73307f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.filter(col(\"listing_id\").isin(listing_to_drop)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d813ea0",
   "metadata": {},
   "source": [
    "1 property has data only for 33 days, while most of other properties obtain data for a whole year (365 days). I will drop this property and other 5 that has data only for 339 days, as it's only rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows for ids because too much data is missing\n",
    "df_calendar_clean = df_calendar.filter(~col(\"listing_id\").isin(listing_to_drop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3e910-e012-4f18-8ad8-a719672ccb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe_info(df_calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options in col 'available'\n",
    "df_calendar_clean.select('available').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38cccc",
   "metadata": {},
   "source": [
    "It's worth to note that, though price has a US dollar sign, it is in Japanese Yen and a sign must be removed in order to convert data to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e3712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check min/max nights values\n",
    "nights_df = df_calendar_clean.select(col('minimum_nights'), col('maximum_nights')).toPandas()\n",
    "nights_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df_calendar_new = df_calendar \\\n",
    "    .withColumn(\"available\", when(col(\"available\") == \"t\", 1).otherwise(0)) \\\n",
    "    .withColumn(\"price\", regexp_replace(col(\"price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "    .withColumn(\"adjusted_price\", regexp_replace(col(\"adjusted_price\"), \"[\\$,]\", \"\").cast(FloatType())) \\\n",
    "    .withColumn(\"date_unix\", unix_timestamp(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check price col\n",
    "df_calendar_new.select(col('price')).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16444d53-e93b-4830-8e5c-5d6634dc35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse price distribution\n",
    "price_data = df_calendar_new.select('price').toPandas()\n",
    "price_dist = price_data.groupby('price').size().reset_index(name='count').sort_values('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414d72e-28eb-4394-97a6-9b297035cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dist.plot(kind='scatter', x='price', y='count', \n",
    "                legend=False, title='Distribution of values in column \"price\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if there's a difference in cols \"price\" and \"adjusted_price\"\n",
    "df_with_diff = df_calendar_new.withColumn(\"price_difference\", col(\"price\") - col(\"adjusted_price\"))\n",
    "\n",
    "# Filter rows where price_difference is not zero\n",
    "rows_with_difference = df_with_diff.filter(col(\"price_difference\") != 0)\n",
    "\n",
    "# count how many rows with differences\n",
    "rows_with_difference.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_difference.filter(col('adjusted_price')>col('price')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708248db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_difference.filter(col('adjusted_price')<col('price')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee368b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_new.filter(col('adjusted_price')==col('price')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff0054",
   "metadata": {},
   "source": [
    "Since there's no data dictionary, I don't really know for sure what is 'adjusted_price' col, but I will take it as main and save col 'price' to drop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alongside with 'min/maximum_nights' which doesn't look correct\n",
    "col_to_drop = ['price', 'minimum_nights', 'maximum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear unused df-s from memory\n",
    "df_with_diff.unpersist()\n",
    "rows_with_difference.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060867ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142fc785",
   "metadata": {},
   "source": [
    "## Load 2nd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = spark.read.csv(dataset_path_hdfs + \"listings.csv\",\n",
    "    header=True, # 1st line is a header\n",
    "    quote='\"',  \n",
    "    escape='\"', \n",
    "    multiLine=True,  # Handles new lines in fields\n",
    "    inferSchema=True,  # detect data types automatically\n",
    "    ignoreLeadingWhiteSpace=True,  # Ignoring white space in a line\n",
    "    ignoreTrailingWhiteSpace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb40a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output above is messy, let's print it pandas' df\n",
    "df_list.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1123a-da3f-4fc6-9391-a2dafbb5dce4",
   "metadata": {},
   "source": [
    "### Explore and Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7151f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if everithing loaded correctly through schema\n",
    "df_list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cc93d",
   "metadata": {},
   "source": [
    "From this dataset I'll take some info to complete my 1st one. Potentially useful columns are:\n",
    "* neighbourhood_cleansed\n",
    "* host_identity_verified\n",
    "* property_type\n",
    "* instant_bookable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cebe05e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.select('property_type').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbc907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.select('room_type').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308422a",
   "metadata": {},
   "source": [
    "After checking unique values, I see that the feature I want is called 'room_type', while 'property_type' consist of marketing names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfae212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_dataframe_info(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaaaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values in the 'id' column\n",
    "unique_ids_list = df_list.select(\"id\").distinct()\n",
    "\n",
    "print(f'Unique IDs: {unique_ids_list.count()},', unique_ids_list.count()-unique_ids, 'properties more than in calendar data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ac24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns that I want to use to expand calendar df\n",
    "selected_cols = [\n",
    "    'id',\n",
    "    'neighbourhood_cleansed',\n",
    "    'room_type',\n",
    "    'host_identity_verified',\n",
    "    'instant_bookable',\n",
    "]\n",
    "new_df = df_list.select(selected_cols)\n",
    "# Merge new df with selected_cols and df_calendar on col id and listing_id\n",
    "merged_df = new_df.join(df_calendar_new, new_df.id == df_calendar_new.listing_id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a778d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop('listing_id') # dublicated col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing dtypes\n",
    "df = merged_df \\\n",
    "    .withColumn(\"host_identity_verified\", when(col(\"host_identity_verified\") == \"t\", True).otherwise(False).cast(BooleanType())) \\\n",
    "    .withColumn(\"instant_bookable\", when(col(\"instant_bookable\") == \"t\", True).otherwise(False).cast(BooleanType()))\n",
    "\n",
    "merged_df.unpersist()\n",
    "new_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-check dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f672c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busy_times = df.where(col(\"available\") == False)\\\n",
    "                  .withColumn(\"year_month\", concat_ws(\"-\", year(\"date\"), month(\"date\"))) \\\n",
    "                  .select('year_month') \\\n",
    "                  .groupBy(\"year_month\").count().toPandas()\n",
    "\n",
    "df_busy_times['year_month'] = df_busy_times['year_month'].astype('period[M]')\n",
    "df_busy_times.sort_values(['year_month'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15213de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busy_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06038c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busy_times.plot(x='year_month', y='count', kind='bar', legend=False,\n",
    "                      title=\"Occupied properties by month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'month+year' column, calculate the mean of 'price',\n",
    "# and sort the results by 'year-month\n",
    "\n",
    "df_price = df.where(col(\"price\") > 0) \\\n",
    "                  .withColumn(\"year_month\", concat_ws(\"-\", year(\"date\"), month(\"date\"))) \\\n",
    "                  .groupBy(\"year_month\") \\\n",
    "                  .agg(mean(\"price\").alias(\"mean\")) \\\n",
    "                  .toPandas()\n",
    "\n",
    "df_price['year_month'] = df_price['year_month'].astype('period[M]')\n",
    "df_price.sort_values('year_month', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16974f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price.plot(kind='line', \n",
    "                x='year_month', y='mean', \n",
    "                legend=False,\n",
    "                title=\"Mean price per night by Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9b9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list.unpersist()\n",
    "df_calendar.unpersist()\n",
    "# clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a364eb",
   "metadata": {},
   "source": [
    "## Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706eedc",
   "metadata": {},
   "source": [
    "I'd like to add new features regarding date to help algorithm find dependencies:\n",
    "- if date is a weekend\n",
    "- if date is a holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34082562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if date is a weekend\n",
    "df_date = df.withColumn(\"weekends\", dayofweek(col(\"date\")).isin([6, 7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6738e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for holiday detection I use holidays module and user-defined function\n",
    "jp_holidays = holidays.Japan()\n",
    "\n",
    "def is_holiday(date):\n",
    "    return date in jp_holidays\n",
    "\n",
    "holiday_udf = udf(is_holiday, BooleanType())\n",
    "\n",
    "df_date = df_date.withColumn(\"holiday\", holiday_udf(col(\"date\"))).sort('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe_info(df_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd87028-aa95-42fa-b6e5-31f290909c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop += ['id', 'date']\n",
    "col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up \n",
    "# deleting cols that won't be used for training\n",
    "df_model = df_date.drop(*col_to_drop)\n",
    "# rename col\n",
    "df_model = df_model.withColumnRenamed('adjusted_price', 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2d50c-6a2c-4c43-8ea7-6fe49a9ddfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataframe_info(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to HDFS in Parquet format\n",
    "df_model.write.parquet(dataset_path_hdfs +\"db\",\n",
    "                       # for re-running this code\n",
    "                       mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bace5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35b2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
