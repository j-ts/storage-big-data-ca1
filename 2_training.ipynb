{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5632a695",
   "metadata": {},
   "source": [
    "# Storage Solutions for Big Data - CA1\n",
    "\n",
    "\n",
    "The assessment CA 1 by **Yulianna Tsaruk** \\\n",
    "Programme Title: Higher Diploma in Science in AI Applications \\\n",
    "Module Title: Storage Solutions for Big Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Code contents:\n",
    "1. **[Exploratory Data Analysis & Processing](./1_processing.ipynb)**\n",
    "2. **Training model and Usage Example (this file)**\n",
    "\n",
    "## Intoduction\n",
    "\n",
    "For this project I'm using HDFS (Hadoop Distributed File System) as the primary storage system, Apache Spark for processing with PySpark - an interface for Apache Spark in Python.\n",
    "\n",
    "In this file I will load the database (previously processed dataset in _file 1_), train a model and use it for prediction on data provided by the user through widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b7930",
   "metadata": {},
   "source": [
    "## Training ML model with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c74562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadbe492-990f-4f17-8ea8-2413e3bdb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to folder with dataset on HDFS\n",
    "dataset_path_hdfs = '/user1/dataset/' # must end with /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cb14c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 17:18:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating Spark session with configurations\n",
    "spark = (SparkSession.builder \\\n",
    "    .appName(\"ML training\")\n",
    "    # hardware-related configs, comment it if not needed for your machine.\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.network.timeout\", \"600s\") \n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\")\n",
    "    \n",
    "    # to output more\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "df = spark.read.parquet(dataset_path_hdfs + \"db\")\n",
    "# values for widgets\n",
    "property_types = df.select('room_type').distinct().toPandas()['room_type'].to_list()\n",
    "neighbourhoods = df.select('neighbourhood_cleansed').distinct().toPandas().sort_values('neighbourhood_cleansed')['neighbourhood_cleansed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077e951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types and  names\n",
    "columns = {\n",
    "    \"continuous\": [\"price\", \"date_unix\"],\n",
    "    \"categorical\": [\"room_type\", \"neighbourhood_cleansed\"],\n",
    "    \"boolean\": [\"instant_bookable\", \"host_identity_verified\", \"weekends\", \"holiday\"],\n",
    "    \"target\": \"available\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e5995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transormers and pipeline\n",
    "class ML():\n",
    "    def __init__(self):\n",
    "        self.assembler_last = None\n",
    "        self.pipeline = None\n",
    "        self.model = None\n",
    "        self.Evaluator = None\n",
    "        \n",
    "    \n",
    "    def make_pipeline(self, columns:dict, maxBins=100):\n",
    "        # Categorical\n",
    "        indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\") \n",
    "                        for col in columns[\"categorical\"]]\n",
    "        # Continuous\n",
    "        vector_assemblers = [VectorAssembler(inputCols=[col], outputCol=f\"{col}_vec\")\n",
    "                                 for col in columns[\"continuous\"]]\n",
    "        # Scaler\n",
    "        scalers = [StandardScaler(inputCol=f\"{col}_vec\", outputCol=f\"scaled_{col}\", withStd=True, withMean=True) \n",
    "                       for col in columns[\"continuous\"]]\n",
    "        # Assemble to one vector\n",
    "        self.assembler_last = VectorAssembler(\n",
    "                        inputCols=[f\"scaled_{col}\" for col in columns[\"continuous\"]] +\n",
    "                        columns[\"boolean\"] +\n",
    "                        [f\"{col}_indexed\" for col in columns[\"categorical\"]],\n",
    "                        outputCol=\"all_features\")\n",
    "        # Define random forest classifier\n",
    "        classifier = RandomForestClassifier(labelCol=columns[\"target\"],\n",
    "                                            featuresCol=\"all_features\",\n",
    "                                            maxBins=maxBins, # max no of categories in categorical values\n",
    "                                            seed=42)\n",
    "        # Define Evaluator for later\n",
    "        self.Evaluator = BinaryClassificationEvaluator(labelCol=columns[\"target\"])\n",
    "        # Define pipeline\n",
    "        self.pipeline = Pipeline(stages=indexers+vector_assemblers+scalers+[self.assembler_last, classifier])\n",
    "\n",
    "    def train(self, data):\n",
    "        self.model = self.pipeline.fit(data)\n",
    "\n",
    "    def pred_eval(self, test_data):\n",
    "        # Make predictions\n",
    "        predictions = self.model.transform(test_data)  \n",
    "        # Evaluate the model\n",
    "        accuracy = self.Evaluator.evaluate(predictions)\n",
    "        auc = self.Evaluator.evaluate(predictions, {self.Evaluator.metricName: 'areaUnderROC'})\n",
    "        print(f\"Accuracy: {accuracy*100:.2f}%, AUC: {auc:.2f}\")\n",
    "    \n",
    "    def find_and_plot_FI(self):\n",
    "        # Find and plot feature importances\n",
    "        feature_names = self.assembler_last.getInputCols()        \n",
    "        feature_importance_dict = dict(zip(feature_names, self.model.stages[-1].featureImportances.toArray()))\n",
    "        imp = pd.DataFrame(sorted(feature_importance_dict.items(), \n",
    "                                  key=lambda x: x[1], reverse=True),\n",
    "                           columns=['feature', 'importance'])\n",
    "        imp.sort_values('importance', inplace=True, ascending=False)\n",
    "        imp['importance'] = imp['importance'] * 100 # normalize values\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = sns.barplot(x=\"importance\", y=\"feature\", hue=\"feature\", data=imp, legend=False)\n",
    "        plt.title('Feature importance of model', fontweight=\"bold\", fontsize=14)\n",
    "        # Annotate values on top of each bar\n",
    "        for container in ax.containers:\n",
    "          rects = container.get_children()\n",
    "          for rect in rects:\n",
    "            width = rect.get_width()\n",
    "            x_loc = width + 0.1\n",
    "            y_loc = rect.get_y() + rect.get_height() / 2\n",
    "            label = f\"{width:.1f}%\"  # Format value with one decimal place\n",
    "            ax.text(x_loc, y_loc, label, ha='left', va='center', fontsize=12)\n",
    "        plt.show()\n",
    "        # ax.bar_label(ax.containers[0])\n",
    "        return imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68991c2f-b839-4968-8fed-26ae7e0446c7",
   "metadata": {},
   "source": [
    "### First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df8359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance\n",
    "pl = ML()\n",
    "pl.make_pipeline(columns, maxBins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9afb306a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[neighbourhood_cleansed: string, room_type: string, host_identity_verified: boolean, instant_bookable: boolean, available: int, price: float, date_unix: bigint, weekends: boolean, holiday: boolean]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "df.unpersist() # clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 17:20:34 WARN MemoryStore: Not enough space to cache rdd_78_0 in memory! (computed 99.9 MiB so far)\n",
      "24/04/27 17:20:34 WARN BlockManager: Persisting block rdd_78_0 to disk instead.\n",
      "24/04/27 17:20:41 WARN MemoryStore: Not enough space to cache rdd_78_1 in memory! (computed 236.8 MiB so far)\n",
      "24/04/27 17:20:41 WARN BlockManager: Persisting block rdd_78_1 to disk instead.\n",
      "24/04/27 17:20:50 WARN MemoryStore: Not enough space to cache rdd_78_0 in memory! (computed 149.9 MiB so far)\n",
      "24/04/27 17:20:51 WARN MemoryStore: Not enough space to cache rdd_78_1 in memory! (computed 236.8 MiB so far)\n",
      "24/04/27 17:20:57 WARN MemoryStore: Not enough space to cache rdd_78_0 in memory! (computed 149.9 MiB so far)\n",
      "24/04/27 17:20:57 WARN MemoryStore: Not enough space to cache rdd_78_1 in memory! (computed 236.8 MiB so far)\n",
      "24/04/27 17:21:03 WARN MemoryStore: Not enough space to cache rdd_78_0 in memory! (computed 149.9 MiB so far)\n",
      "24/04/27 17:21:04 WARN MemoryStore: Not enough space to cache rdd_78_1 in memory! (computed 236.8 MiB so far)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pl.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680888d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "pl.pred_eval(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13295a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "pl.find_and_plot_FI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b7f3b",
   "metadata": {},
   "source": [
    "### Second model - Retrain\n",
    "Let's reduce dimentiality by getting rid of features with importance less that 2% and re-train model. This will require less preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899d2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_col = ['weekends','holiday']\n",
    "\n",
    "train_data, test_data = train_data.drop(*drop_col), test_data.drop(*drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f67392-9866-4f44-b4de-fd208b14a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-define column types and  names\n",
    "columns = {\n",
    "    \"continuous\": [\"price\", \"date_unix\"],\n",
    "    \"categorical\": [\"room_type\", \"neighbourhood_cleansed\"],\n",
    "    \"boolean\": [\"instant_bookable\", \"host_identity_verified\"],\n",
    "    \"target\": \"available\"\n",
    "}\n",
    "# Re-train a model\n",
    "pl = ML()\n",
    "pl.make_pipeline(columns, maxBins=55)\n",
    "pl.train(train_data)\n",
    "pl.pred_eval(test_data)\n",
    "pl.find_and_plot_FI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5149b58-014b-409a-a43e-114d8c018631",
   "metadata": {},
   "source": [
    "Metrics are slightly improved, and _room_type_ feature gained more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7fa0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model to variable\n",
    "model = pl.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75052d",
   "metadata": {},
   "source": [
    "## Usage Example of Model trained on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6126f6-838d-4e4e-9606-5cd15756bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date picker\n",
    "date_picker = widgets.DatePicker(\n",
    "    description='Pick a Date',\n",
    "    disabled=False,\n",
    "    value=datetime.date.today())\n",
    "# Property type dropdown\n",
    "property_type_dropdown = widgets.Dropdown(\n",
    "    options=property_types,\n",
    "    description='Property Type:',\n",
    "    disabled=False,)\n",
    "# Neighborhood dropdown\n",
    "neighborhood_dropdown = widgets.Dropdown(\n",
    "    options=neighbourhoods,\n",
    "    description='Neighborhood:',\n",
    "    disabled=False,)\n",
    "# Price \n",
    "price_input = widgets.FloatText(\n",
    "    description='Price per night (in Yen):',\n",
    "    value=10000.0,\n",
    "    step=1.0,\n",
    "    continuous_update=False)\n",
    "# Instant bookable \n",
    "instant_bookable_toggle = widgets.ToggleButtons(\n",
    "    options=['Yes', 'No'],\n",
    "    description='Instant Bookable:',\n",
    "    disabled=False)\n",
    "# Verified host \n",
    "verified_host_toggle = widgets.ToggleButtons(\n",
    "    options=['Yes', 'No'],\n",
    "    description='Verified Host:',\n",
    "    disabled=False)\n",
    "# progress bar\n",
    "progress = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    description='Predicting...\\n',\n",
    "    bar_style='info',\n",
    "    orientation='horizontal')\n",
    "\n",
    "progress.layout.visibility = 'hidden'\n",
    "output_w = widgets.Output()\n",
    "\n",
    "New_DF = None # global variable to store new data from user\n",
    "\n",
    "# to display all widgets\n",
    "def display_widgets():\n",
    "    display(date_picker, property_type_dropdown, neighborhood_dropdown, price_input, instant_bookable_toggle, verified_host_toggle)\n",
    "\n",
    "# to predict and output result\n",
    "def make_prediction():\n",
    "    output_w.clear_output(wait=True)\n",
    "    progress.layout.visibility = 'visible'\n",
    "    progress.value = 0\n",
    "    \n",
    "    with output_w:\n",
    "        progress.value = 1\n",
    "        # define spark DF\n",
    "        df = spark.createDataFrame(New_DF)\n",
    "        # Make predictions on new data\n",
    "        predictions = model.transform(df)\n",
    "        progress.value = 4\n",
    "        result = predictions.select(\"prediction\").toPandas()['prediction'][0]\n",
    "        proba = predictions.select(\"probability\").toPandas()['probability'][0][int(result)]\n",
    "        progress.value = 5\n",
    "        input_data = f\"\"\"\n",
    "        <p style=\"line-height: 0\">Data used:</p>\n",
    "        <ul>\n",
    "        <li>Date: <strong>{date_picker.value}</strong></li>\n",
    "        <li>Price per night: <strong>{price_input.value} ¥</strong></li>\n",
    "        <li>Property type: <strong>{property_type_dropdown.value}</strong></li>\n",
    "        <li>Neighbourhood: <strong>{neighborhood_dropdown.value}</strong></li>\n",
    "        <li>Instant bookable: <strong>{instant_bookable_toggle.value}</strong></li>\n",
    "        <li>Verified host: <strong>{verified_host_toggle.value}</strong></li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "        if result > 0:\n",
    "            string = f'<h3>Your property will be occupied with {proba*100:.1f}% confidence.</h3>'\n",
    "        else:\n",
    "            string = f'<h3>Your property will be available with {(proba*100):.1f}% confidence.</h3>'\n",
    "        progress.value = 8\n",
    "        progress.layout.visibility = 'hidden'\n",
    "        # Display the result\n",
    "        display(widgets.HTML(value=string + input_data))\n",
    "\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global New_DF\n",
    "    # change time to unix timestamp\n",
    "    unix_timestamp = int(time.mktime(date_picker.value.timetuple()))    \n",
    "    data = {\n",
    "        'date_unix': [unix_timestamp],\n",
    "        'price': [price_input.value],\n",
    "        'room_type': [property_type_dropdown.value],\n",
    "        'neighbourhood_cleansed': [neighborhood_dropdown.value],\n",
    "        'instant_bookable': [instant_bookable_toggle.value == 'Yes'],\n",
    "        'host_identity_verified': [verified_host_toggle.value == 'Yes']\n",
    "    }\n",
    "    New_DF = pd.DataFrame(data)\n",
    "    # hide widgets\n",
    "    date_picker.close()\n",
    "    property_type_dropdown.close()\n",
    "    price_input.close()\n",
    "    neighborhood_dropdown.close()\n",
    "    instant_bookable_toggle.close()\n",
    "    verified_host_toggle.close()\n",
    "    b.close()\n",
    "    make_prediction()\n",
    "\n",
    "display_widgets()\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "display(submit_button, progress, output_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c423b0-7a01-413a-806e-e5d3287cc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ace7e-749b-425b-afcf-5a8b5d8473bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
